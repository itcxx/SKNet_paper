import torch
import torch.nn as nn
import torch.nn.functional as F
import torch
import torch.nn as nn
import torch.nn.functional as F

# class ChannelContrastiveLoss(nn.Module):
#     """
#     é€šé“çº§å¯¹æ¯”æŸå¤±ï¼Œä½¿ç”¨ soft-argmax ä» pred å’Œ target heatmaps ä¸­æå– peak åæ ‡å¹¶æ„å»º triplet-style lossã€‚
#     è¾“å…¥:
#         - pred_heatmaps: (B, K, D, H, W)
#         - target_heatmaps: (B, K, D, H, W)
#     """
#     def __init__(self, margin=4.0, temperature=0.05):
#         super().__init__()
#         self.margin = margin
#         self.temperature = temperature
#
#     def soft_argmax_3d(self, heatmap):
#         """
#         3D soft-argmax å®ç°ã€‚
#         è¾“å…¥: (K, D, H, W) æˆ– (1, K, D, H, W)
#         è¾“å‡º: (K, 3) => (x, y, z)
#         """
#         if heatmap.dim() == 4:
#             heatmap = heatmap.unsqueeze(0)  # (1, K, D, H, W)
#
#         N, K, D, H, W = heatmap.shape
#         heatmap = heatmap.view(N, K, -1)
#         heatmap = heatmap - heatmap.amax(dim=2, keepdim=True)
#         heatmap = F.softmax(heatmap / self.temperature, dim=2)
#         heatmap = heatmap.view(N, K, D, H, W)
#
#         device = heatmap.device
#         z_range = torch.linspace(0, D - 1, D, device=device)
#         y_range = torch.linspace(0, H - 1, H, device=device)
#         x_range = torch.linspace(0, W - 1, W, device=device)
#         zz, yy, xx = torch.meshgrid(z_range, y_range, x_range, indexing='ij')
#
#         xx = xx.view(1, 1, D, H, W)
#         yy = yy.view(1, 1, D, H, W)
#         zz = zz.view(1, 1, D, H, W)
#
#         x = torch.sum(heatmap * xx, dim=(2, 3, 4))
#         y = torch.sum(heatmap * yy, dim=(2, 3, 4))
#         z = torch.sum(heatmap * zz, dim=(2, 3, 4))
#
#         return torch.stack([x, y, z], dim=2).squeeze(0)  # (K, 3)
#
#     def forward(self, pred_heatmaps, target_heatmaps):
#         """
#         pred_heatmaps, target_heatmaps: (B, K, D, H, W)
#         """
#         B, K, D, H, W = pred_heatmaps.shape
#         total_loss = 0.0
#
#         for b in range(B):
#             pred = pred_heatmaps[b]      # (K, D, H, W)
#             target = target_heatmaps[b]  # (K, D, H, W)
#
#             pred_kpts = self.soft_argmax_3d(pred)     # (K, 3)
#             target_kpts = self.soft_argmax_3d(target) # (K, 3)
#
#             for k in range(K):
#                 gt_k = target_kpts[k]
#                 pred_k = pred_kpts[k]
#                 loss_pos = F.mse_loss(pred_k, gt_k)
#
#                 loss_neg = 0.0
#                 for j in range(K):
#                     if j == k:
#                         continue
#                     pred_j = pred_kpts[j]
#                     d_pos = F.mse_loss(pred_k, gt_k)
#                     d_neg = F.mse_loss(pred_j, gt_k)
#                     loss_neg += F.relu(self.margin - (d_neg - d_pos))
#
#                 total_loss += loss_pos + loss_neg / (K - 1)
#
#         return total_loss / B
class ChannelContrastiveLoss(nn.Module):
    def __init__(self, margin=4.0, temperature=0.05):
        super().__init__()
        self.margin = margin
        self.temperature = temperature

    def soft_argmax_3d(self, heatmaps):
        """
        è¾“å…¥: heatmaps: (B, K, D, H, W)
        è¾“å‡º: keypoints: (B, K, 3) => (x, y, z)
        """
        B, K, D, H, W = heatmaps.shape
        heatmaps = heatmaps.view(B, K, -1)
        heatmaps = heatmaps - heatmaps.amax(dim=2, keepdim=True)
        heatmaps = F.softmax(heatmaps / self.temperature, dim=2)
        heatmaps = heatmaps.view(B, K, D, H, W)

        device = heatmaps.device
        z_range = torch.linspace(0, D - 1, D, device=device)
        y_range = torch.linspace(0, H - 1, H, device=device)
        x_range = torch.linspace(0, W - 1, W, device=device)
        zz, yy, xx = torch.meshgrid(z_range, y_range, x_range, indexing='ij')

        zz = zz[None, None, :, :, :].to(device)
        yy = yy[None, None, :, :, :].to(device)
        xx = xx[None, None, :, :, :].to(device)

        x = torch.sum(heatmaps * xx, dim=(2, 3, 4))
        y = torch.sum(heatmaps * yy, dim=(2, 3, 4))
        z = torch.sum(heatmaps * zz, dim=(2, 3, 4))

        keypoints = torch.stack([x, y, z], dim=2)  # (B, K, 3)
        return keypoints

    def forward(self, pred_heatmaps, target_heatmaps):
        """
        pred_heatmaps, target_heatmaps: (B, K, D, H, W)
        è¿”å›: scalar loss
        """
        B, K, D, H, W = pred_heatmaps.shape
        pred_kpts = self.soft_argmax_3d(pred_heatmaps)     # (B, K, 3)
        target_kpts = self.soft_argmax_3d(target_heatmaps) # (B, K, 3)

        # æ­£æ ·æœ¬è·ç¦»
        loss_pos = F.mse_loss(pred_kpts, target_kpts, reduction='none')  # (B, K, 3)
        loss_pos = loss_pos.mean(dim=2)  # (B, K)

        # è´Ÿæ ·æœ¬è·ç¦»ï¼ˆæ„å»ºå¯¹æ¯”é¡¹ï¼‰
        # Expand to (B, K, K, 3)
        pred_kpts_i = pred_kpts.unsqueeze(2)  # (B, K, 1, 3)
        pred_kpts_j = pred_kpts.unsqueeze(1)  # (B, 1, K, 3)
        target_kpts_i = target_kpts.unsqueeze(2)  # (B, K, 1, 3)

        d_pos = F.mse_loss(pred_kpts_i, target_kpts_i, reduction='none').mean(dim=3)  # (B, K, 1)
        d_neg = F.mse_loss(pred_kpts_j, target_kpts_i, reduction='none').mean(dim=3)  # (B, K, K)

        # å±è”½å¯¹è§’çº¿ï¼ˆj == kï¼Œæ­£æ ·æœ¬ï¼‰
        eye = torch.eye(K, device=pred_heatmaps.device).unsqueeze(0)  # (1, K, K)
        mask = 1.0 - eye  # (1, K, K)
        d_neg_masked = d_neg * mask  # (B, K, K)

        # Triplet loss: relu(margin - (d_neg - d_pos))
        triplet_loss = F.relu(self.margin - (d_neg_masked - d_pos))  # (B, K, K)
        loss_neg = triplet_loss.sum(dim=2) / (K - 1)  # å¹³å‡è´Ÿæ ·æœ¬æŸå¤± (B, K)

        total_loss = (loss_pos + loss_neg).mean()  # batch + joints å¹³å‡
        return total_loss
# class KeypointMSELoss(nn.Module):
#     """ç±»ä¼¼ Interhand çš„ KeypointMSELoss, ç”¨äºçƒ­å›¾çš„ MSE å›å½’.
#
#     Args:
#         use_target_weight (bool): æ˜¯å¦ä½¿ç”¨å…³é”®ç‚¹æƒé‡, é»˜è®¤ False
#         skip_empty_channel (bool): æ˜¯å¦è·³è¿‡å…¨éƒ¨ä¸º0çš„é€šé“, é»˜è®¤ False
#         loss_weight (float): æŸå¤±æ•´ä½“æƒé‡, é»˜è®¤ 1.0
#     """
#
#     def __init__(self,
#                  use_target_weight: bool = False,
#                  skip_empty_channel: bool = False,
#                  loss_weight: float = 1.0):
#         super().__init__()
#         self.use_target_weight = use_target_weight
#         self.skip_empty_channel = skip_empty_channel
#         self.loss_weight = loss_weight
#
#     def forward(self,
#                 output: torch.Tensor,
#                 target: torch.Tensor,
#                 target_weights: torch.Tensor = None,
#                 mask: torch.Tensor = None) -> torch.Tensor:
#         """
#         è®¡ç®— MSE Loss (B,K,H,W) å½¢çŠ¶çš„çƒ­å›¾.
#
#         Args:
#             output (Tensor): æ¨¡å‹è¾“å‡ºçš„çƒ­å›¾, shape = [B, K, H, W]
#             target (Tensor): GTçƒ­å›¾, shape = [B, K, H, W]
#             target_weights (Tensor, optional):
#                 - å¦‚æœ shape = [B, K], è¡¨ç¤ºæ¯ä¸ªå…³èŠ‚çš„æƒé‡
#                 - å¦‚æœ shape = [B, K, H, W], è¡¨ç¤ºæ¯ä¸ªåƒç´ çº§åˆ«çš„æƒé‡
#             mask (Tensor, optional):
#                 - ç©ºé—´æ©ç , shape å¯ä»¥æ˜¯ [B, K, H, W] æˆ– [B, 1, H, W].
#                   1 è¡¨ç¤ºæœ‰æ•ˆ, 0 è¡¨ç¤ºæ— æ•ˆ.
#
#         Returns:
#             Tensor: æ ‡é‡æŸå¤±, shape []
#         """
#
#         # 1) æ„é€ æœ€ç»ˆ mask
#         final_mask = self._get_mask(target, target_weights, mask)
#
#         # 2) è‹¥æ— ä»»ä½• mask, ç›´æ¥ç”¨ mse_loss
#         if final_mask is None:
#             loss = F.mse_loss(output, target, reduction='mean')
#         else:
#             # å¦åˆ™ å…ˆ element-wise è®¡ç®— mse, å†ä¹˜ mask å mean
#             _loss = F.mse_loss(output, target, reduction='none')  # ä¿ç•™ [B,K,H,W]
#             loss = (_loss * final_mask).mean()  # å†åšå‡å€¼
#
#         return loss * self.loss_weight
#
#     def _get_mask(self,
#                   target: torch.Tensor,
#                   target_weights: torch.Tensor = None,
#                   mask: torch.Tensor = None) -> torch.Tensor:
#         """
#         ç”Ÿæˆæœ€ç»ˆçš„ mask, ç»“åˆ:
#           - è¾“å…¥çš„ mask
#           - target_weights
#           - skip_empty_channel
#         è‹¥æœ€ç»ˆæ²¡æœ‰ä»»ä½•æœ‰æ•ˆå±è”½, åˆ™è¿”å› None.
#         """
#
#         # ------------------------
#         # a) å’Œ target shape å¯¹é½
#         # ------------------------
#         final_mask = mask
#         # å¦‚æœ mask ä¸ä¸ºç©º, æ£€æŸ¥å®ƒç»´åº¦æ˜¯å¦èƒ½ broadcast åˆ° target
#         if final_mask is not None:
#             assert final_mask.dim() == target.dim(), (
#                 f'Maskå½¢çŠ¶ä¸targetä¸åŒ¹é…: mask={final_mask.shape}, target={target.shape}')
#
#         # ------------------------
#         # b) target_weights
#         # å¦‚æœ use_target_weight=True æˆ– è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¯è§æ€§åŠ æƒ
#         # ------------------------
#         if self.use_target_weight and (target_weights is not None):
#             # target_weights çš„ shape å¯èƒ½æ˜¯ (B,K) æˆ– (B,K,H,W)
#             if target_weights.dim() == 2:
#                 # (B,K) -> (B,K,1,1)
#                 target_weights = target_weights[..., None, None]
#             # å¹¿æ’­åˆ° (B,K,H,W)
#             if final_mask is None:
#                 final_mask = target_weights
#             else:
#                 final_mask = final_mask * target_weights
#
#         # ------------------------
#         # c) skip_empty_channel
#         # å¦‚æœå¯ç”¨, åˆ™å¯¹äºä¸€ä¸ªé€šé“é‡Œå…¨æ˜¯0çš„GT, ç›´æ¥è·³è¿‡.
#         # ------------------------
#         if self.skip_empty_channel:
#             # æ‰¾åˆ° non-zero çš„é€šé“
#             # shape (B,K,H,W) -> flatten(2) => (B,K, H*W)
#             # any(dim=2) => (B,K)
#             non_zero_map = (target != 0).flatten(2).any(dim=2)  # bool, shape=(B,K)
#             # å† reshape å›( B,K,1,1 ) ä»¥ä¾¿å¹¿æ’­
#             non_zero_map = non_zero_map[..., None, None]  # (B,K,1,1)
#
#             if final_mask is None:
#                 final_mask = non_zero_map.float()
#             else:
#                 final_mask = final_mask * non_zero_map.float()
#
#         # å¦‚æœæœ€ç»ˆæ²¡æœ‰å¾—åˆ°ä»»ä½• mask, åˆ™è¿”å› None
#         if final_mask is None:
#             return None
#
#         return final_mask
class Heatmap3DHead(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        depth_size: int = 64,
        deconv_out_channels=(256, 256, 256),
        deconv_kernel_sizes=(4, 4, 4),
        add_last_bnrelu: bool = False  # âœ… æ§åˆ¶æœ€åä¸€å±‚æ˜¯å¦åŠ BN+ReLU
    ):
        super().__init__()
        assert out_channels % depth_size == 0, "out_channels å¿…é¡»æ˜¯ depth_size çš„å€æ•°"
        self.depth_size = depth_size

        deconv_layers = []
        prev_channels = in_channels

        for i, (out_c, kernel) in enumerate(zip(deconv_out_channels, deconv_kernel_sizes)):
            deconv_layers.append(
                nn.ConvTranspose2d(
                    prev_channels,
                    out_c,
                    kernel_size=kernel,
                    stride=2,
                    padding=1,
                    output_padding=0,
                    bias=False
                )
            )
            is_last = (i == len(deconv_out_channels) - 1)
            if not is_last or (is_last and add_last_bnrelu):
                deconv_layers.append(nn.BatchNorm2d(out_c))
                deconv_layers.append(nn.ReLU(inplace=True))

            prev_channels = out_c

        self.deconv_layers = nn.Sequential(*deconv_layers)

        # final conv to output heatmaps
        self.final_layer = nn.Conv2d(prev_channels, out_channels, kernel_size=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.deconv_layers(x)  # upsample
        x = self.final_layer(x)    # channel -> out_channels
        N, C, H, W = x.shape
        x = x.view(N, C // self.depth_size, self.depth_size, H, W)  # (N, K, D, H, W)

        # x = torch.sigmoid(x)  # optional, depends on your label range (0~1)  , 2025/04/08-2 Test01
        # x = F.softplus(x) / (F.softplus(x).max() + 1e-6) # 2025/04/08-2 Test02

        return x
class Heatmap1DHead(nn.Module):
    """Heatmap1DHead: é¢„æµ‹ç›¸å¯¹æ ¹æ·±åº¦çš„ 1D çƒ­åŠ›å›¾

    Args:
        in_channels (int): è¾“å…¥é€šé“æ•°.
        heatmap_size (int): 1D çƒ­åŠ›å›¾å¤§å°.
        hidden_dims (tuple[int]): å…¨è¿æ¥å±‚çš„éšè—å•å…ƒæ•°.
    """

    def __init__(self, 
                 in_channels: int = 2048, 
                 heatmap_size: int = 64, 
                 hidden_dims=(512, )):
        super().__init__()

        self.in_channels = in_channels
        self.heatmap_size = heatmap_size

        # å…¨è¿æ¥å±‚
        layers = []
        prev_dim = in_channels
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU(inplace=True))
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, heatmap_size))
        self.fc = nn.Sequential(*layers)

    def soft_argmax_1d(self, heatmap1d):
        """1D Soft-Argmax è®¡ç®—æ ¹æ·±åº¦"""
        heatmap1d = F.softmax(heatmap1d, dim=1)  # å½’ä¸€åŒ–
        accu = heatmap1d * torch.arange(self.heatmap_size, dtype=heatmap1d.dtype, device=heatmap1d.device)[None, :]
        coord = accu.sum(dim=1)
        return coord

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.shape[0], -1)
        x = self.fc(x)
        return self.soft_argmax_1d(x).view(-1, 1)

    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.fc.modules():
            if isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
class MultilabelClassificationHead(nn.Module):
    """é¢„æµ‹æ‰‹éƒ¨ç±»å‹ (å·¦æ‰‹/å³æ‰‹)"""

    def __init__(self, in_channels, num_labels=2):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(in_channels, 512),
            nn.ReLU(),
            nn.Linear(512, num_labels)
        )

    def forward(self, x):
        x = F.adaptive_avg_pool2d(x, (1, 1)).view(x.shape[0], -1)
        return self.fc(x)
class HandPoseHead(nn.Module):
    """3D æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹ Head"""

    def __init__(self, in_channels, num_joints, depth_size, heatmap_size):
        super().__init__()
        self.heatmap_head = Heatmap3DHead(in_channels, num_joints*depth_size, depth_size)
        self.root_head = Heatmap1DHead(in_channels)
        self.hand_type_head = MultilabelClassificationHead(in_channels)
        self.heatmap_size = heatmap_size  # (D, H, W)

        self.loss_kpt = nn.MSELoss(reduction="none")
        # self.loss_kpt = KeypointMSELoss(
        #     use_target_weight=False,
        #     skip_empty_channel=True,
        #     loss_weight=1.0
        # )
        self.loss_kpt_xy = nn.MSELoss()  # é¢å¤–çš„ (x, y) ä½ç½®çº¦æŸæŸå¤±
        self.loss_root = nn.L1Loss()
        self.loss_hand_type = nn.BCEWithLogitsLoss()

    def forward(self, x):
        heatmaps = self.heatmap_head(x)  # 3D å…³é”®ç‚¹çƒ­åŠ›å›¾
        root_depth = self.root_head(x)  # æ ¹éƒ¨æ·±åº¦é¢„æµ‹
        hand_type = self.hand_type_head(x)  # æ‰‹éƒ¨ç±»åˆ«é¢„æµ‹
        return heatmaps, root_depth, hand_type

    @staticmethod
    def get_heatmap_3d_maximum(heatmaps: torch.Tensor, image_size=None, depth_bound=400):
        """
        ä» 3D çƒ­å›¾ä¸­æå–æœ€å¤§å“åº”ä½ç½®ï¼Œä½œä¸ºé¢„æµ‹å…³é”®ç‚¹çš„åæ ‡ã€‚

        è¾“å…¥:
          - heatmaps: (N, K, D, H, W) - 3D å…³é”®ç‚¹çƒ­å›¾
          - image_size: tuple (width, height)ï¼Œè‹¥ä¸º Noneï¼Œåˆ™è¿”å›çƒ­å›¾å°ºåº¦ä¸‹çš„ç‚¹ä½ç½®

        è¾“å‡º:
          - keypoints: (N, K, 3) -> (x, y, z)
          - scores: (N, K) -> å¯¹åº”å…³é”®ç‚¹çš„ç½®ä¿¡åº¦
        """
        N, K, D, H, W = heatmaps.shape
        scores, indices = torch.max(heatmaps.view(N, K, -1), dim=2)
        depth_bound = depth_bound  # è®¾å®š z å½’ä¸€åŒ–èŒƒå›´
        # è½¬æ¢ indices ä¸ºä¸‰ç»´åæ ‡ (x, y, z)
        x_heat = indices % W  # çƒ­å›¾ x åæ ‡
        y_heat = (indices // W) % H  # çƒ­å›¾ y åæ ‡
        z_heat = indices // (W * H)  # çƒ­å›¾ z åæ ‡

        # è‹¥ä¸æä¾›åŸå›¾å°ºå¯¸ï¼Œåˆ™è¿”å›çƒ­å›¾åæ ‡
        if image_size is None:
            keypoints = torch.stack((x_heat, y_heat, z_heat), dim=-1).float()
            return keypoints, scores

        # è‹¥æä¾› image_sizeï¼Œåˆ™è½¬æ¢åˆ°åŸå§‹å›¾åƒåæ ‡
        img_w, img_h = image_size

        keypoints = torch.zeros((N, K, 3), dtype=torch.float32, device=heatmaps.device)
        keypoints[:, :, 0] = x_heat.float() * img_w / W
        keypoints[:, :, 1] = y_heat.float() * img_h / H
        keypoints[:, :, 2] = ((z_heat.float() / D) - 0.5) * depth_bound

        return keypoints, scores

    def soft_argmax_2d(self,heatmap_3d , T = 0.001):
        # heatmap_3d: (N, K, D, H, W)
        N, K, D, H, W = heatmap_3d.shape

        # æ±‚ z æ–¹å‘ä¸Šçš„æœ€å¤§å€¼ï¼ˆèšåˆï¼‰
        xy_heatmap = torch.sum(heatmap_3d, dim=2)  # (N, K, H, W)
        xy_heatmap = xy_heatmap.view(N, K, -1)
        xy_heatmap = xy_heatmap - xy_heatmap.amax(dim=2, keepdim=True)  # âœ… ç¨³å®š ############
        xy_heatmap = F.softmax(xy_heatmap / T, dim=2)  # âœ… dim=2 æ˜¯ (HÃ—W) å†…éƒ¨
        xy_heatmap = xy_heatmap.view(N, K, H, W)

        # æ„é€ åæ ‡ç½‘æ ¼
        device = heatmap_3d.device
        x_coords = torch.arange(W, device=device).float()
        y_coords = torch.arange(H, device=device).float()
        y_grid, x_grid = torch.meshgrid(y_coords, x_coords, indexing='ij')  # (H, W)

        x_grid = x_grid.view(1, 1, H, W)
        y_grid = y_grid.view(1, 1, H, W)

        x = (xy_heatmap * x_grid).sum(dim=(2, 3))  # (N, K)
        y = (xy_heatmap * y_grid).sum(dim=(2, 3))  # (N, K)

        return torch.stack([x, y], dim=-1)  # (N, K, 2)

    @staticmethod
    def soft_argmax_3d(heatmap_3d , T = 0.05):
        """
        æ”¯æŒè¾“å…¥ä¸º np.ndarray æˆ– torch.Tensor çš„ soft-argmax 3D å®ç°ã€‚

        Args:
            heatmap_3d (Union[np.ndarray, Tensor]): (N, K, D, H, W) æˆ– (K, D, H, W)

        Returns:
            keypoints_3d (Tensor): (N, K, 3) -> (x, y, z)
            scores (Tensor): (N, K) -> æ¯ä¸ªå…³é”®ç‚¹çš„ç½®ä¿¡åº¦
        """
        import numpy as np
        import torch
        import torch.nn.functional as F

        # è½¬ä¸º Tensor
        if isinstance(heatmap_3d, np.ndarray):
            heatmap_3d = torch.from_numpy(heatmap_3d).float()

        # è¡¥å…… batch ç»´åº¦
        if heatmap_3d.dim() == 4:  # (K, D, H, W) â†’ (1, K, D, H, W)
            heatmap_3d = heatmap_3d.unsqueeze(0)

        N, K, D, H, W = heatmap_3d.shape

        # Softmax
        # heatmap = heatmap_3d.view(N * K, -1)
        # heatmap = F.softmax(heatmap / T, dim=1)
        # heatmap = heatmap.view(N, K, D, H, W)
        heatmap = heatmap_3d.view(N, K, -1)
        heatmap = heatmap - heatmap.amax(dim=2, keepdim=True)   # âœ… ç¨³å®š
        heatmap = F.softmax(heatmap / T, dim=2)  # âœ… æ¯ä¸ªå…³é”®ç‚¹å†…éƒ¨åš softmax
        heatmap = heatmap.view(N, K, D, H, W)

        device = heatmap.device
        z_range = torch.linspace(0, D - 1, D, device=device)
        y_range = torch.linspace(0, H - 1, H, device=device)
        x_range = torch.linspace(0, W - 1, W, device=device)
        zz, yy, xx = torch.meshgrid(z_range, y_range, x_range, indexing="ij")

        xx = xx.view(1, 1, D, H, W)
        yy = yy.view(1, 1, D, H, W)
        zz = zz.view(1, 1, D, H, W)

        x = torch.sum(heatmap * xx, dim=(2, 3, 4))
        y = torch.sum(heatmap * yy, dim=(2, 3, 4))
        z = torch.sum(heatmap * zz, dim=(2, 3, 4))

        keypoints = torch.stack([x, y, z], dim=2)  # (N, K, 3)
        scores = torch.amax(heatmap, dim=(2, 3, 4))  # (N, K)

        return keypoints, scores

    def compute_loss(self, pred, target):
        """è®¡ç®—æŸå¤±"""
        pred_heatmaps, pred_root, pred_hand_type = pred
        target_heatmaps, target_root, target_hand_type = target

        ######################################################
        # 2025/03/24 loss
        ######################################################
        # # print(f"heatmaps:{target_heatmaps}")
        #
        # # è·å– 3D çƒ­å›¾çš„å³°å€¼ maskï¼ˆåªå¯¹ target ä¸­é«˜äºæŸé˜ˆå€¼çš„åŒºåŸŸè¿›è¡Œç›‘ç£ï¼‰
        # heatmap_mask = (target_heatmaps > 0.005).float()  # âœ… ä½ ä¹Ÿå¯ä»¥å°è¯• 0.01, 0.1 è¿›è¡Œæ¯”è¾ƒ
        # # print(f"heatmap_mask:{heatmap_mask}")
        #
        # # åœ¨è®¡ç®—æŸå¤±å‰ï¼Œå¯¹ target_hand_type åš squeeze æ“ä½œ
        # if target_hand_type.dim() == 3:
        #     target_hand_type = target_hand_type.squeeze(1)
        #
        # # è®¡ç®— 3D heatmap lossï¼ˆä»…åœ¨ mask å†…è®¡ç®—ï¼‰
        # loss_kpt = (self.loss_kpt(pred_heatmaps, target_heatmaps) * heatmap_mask).mean()
        # loss_root = self.loss_root(pred_root, target_root)
        # loss_hand = self.loss_hand_type(pred_hand_type, target_hand_type)
        # # ========== (x, y) ä½ç½®ç›‘ç£ï¼šsoft-argmax 2D  ==========
        # pred_xy = self.soft_argmax_2d(pred_heatmaps)  # (B, K, 2)
        # target_xy = self.soft_argmax_2d(target_heatmaps)  # (B, K, 2) from GT heatmap
        #
        # loss_kpt_xy = self.loss_kpt_xy(pred_xy, target_xy)
        ######################################################
        # 2025/03/25 loss
        ######################################################
        # # -------------------
        # # âœ… soft mask æ›¿ä»£ binary maskï¼šæ›´å¹³æ»‘ã€æ›´ç»“æ„åŒ–ç›‘ç£
        # soft_mask = target_heatmaps  # (N, K, D, H, W)ï¼Œé«˜æ–¯ç»“æ„å°±æ˜¯ weight
        # # -------------------
        # # âœ… é˜²æ­¢ hand_type dim ä¸ä¸€è‡´
        # if target_hand_type.dim() == 3:
        #     target_hand_type = target_hand_type.squeeze(1)
        # # -------------------
        # # âœ… 3D heatmap MSE lossï¼ˆç»“æ„æŸå¤±ï¼‰
        # loss_kpt = (self.loss_kpt(pred_heatmaps, target_heatmaps) * soft_mask).mean()
        # # -------------------
        # # âœ… ä½ç½®çº§åˆ«ç²¾ç»†ç›‘ç£ï¼ˆå¯é€‰ soft-argmaxï¼‰, åœ¨trainä¸­é€‰æ‹©ä»€ä¹ˆæ—¶å€™ä½¿ç”¨è¿™ä¸ªloss
        # pred_xy = self.soft_argmax_2d(pred_heatmaps)
        # target_xy = self.soft_argmax_2d(target_heatmaps)
        # loss_kpt_xy = self.loss_kpt_xy(pred_xy, target_xy)
        # # -------------------
        # # âœ… Root depth L1 loss
        # loss_root = self.loss_root(pred_root, target_root)
        # # -------------------
        # # âœ… Hand type BCE loss
        # loss_hand = self.loss_hand_type(pred_hand_type, target_hand_type)
        ######################################################
        # 2025/04/08 loss åœ¨ä¹‹å‰çš„lossçš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªä¿¡æ¯ç†µæ¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œé˜²æ­¢æ¨¡å‹å­¦ä¹ åˆ°çš„çƒ­å›¾ï¼Œå› ä¸ºloss_xyçš„è°ƒèŠ‚åå¡Œæˆä¸€ä¸ªç‚¹ã€‚
        ######################################################
        # === soft mask æ›¿ä»£ binary maskï¼šæ›´å¹³æ»‘ã€æ›´ç»“æ„åŒ–ç›‘ç£
        soft_mask = target_heatmaps  # (N, K, D, H, W)

        #####################################################
        # 2025/04/10 soft_mask å¯èƒ½æ˜¯å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°çš„Kç»´åº¦çš„çƒ­å›¾å­¦ä¹ åˆ°ä¸€å¿«çš„åŸå› ï¼Œç›´æ¥ä½¿ç”¨binary mask è¿›è¡Œæµ‹è¯•
        #####################################################
        # binary_mask = (target_heatmaps > 1e-3).float()

        #####################################################
        # 2025/04/10-1 soft_mask å¯èƒ½æ˜¯å¯¼è‡´æ¨¡å‹å­¦ä¹ åˆ°çš„Kç»´åº¦çš„çƒ­å›¾å­¦ä¹ åˆ°ä¸€å¿«çš„åŸå› ï¼Œç›´æ¥ä¸ç”¨mask è¿›è¡Œè®­ç»ƒæµ‹è¯•
        #####################################################
        #

        #####################################################
        # 2025/04/11 æŒ‰ç…§ batch ä¸­ä¸åŒæ ·æœ¬ï¼Œè½®æµç›‘ç£ä¸åŒçš„ joint channel
        #####################################################

        #####################################################
        # 2025/04/13 æ¯”è¾ƒå­¦ä¹ æ–¹å¼çš„é€šé“å±‚æ¬¡ç›‘ç£
        #####################################################
        # contrastive_loss = ChannelContrastiveLoss(margin=4.0)
        # loss_contrastive = contrastive_loss(pred_heatmaps, target_heatmaps)

        #####################################################
        # 2025/04/14  soft_mask å®¹æ˜“å¯¼è‡´æ¨¡å‹å­¦åˆ°æ•´å›¾è€¦åˆç‰¹å¾, åŠ å…¥ tail regularizationï¼ˆå°¾éƒ¨æ­£åˆ™ï¼‰
        #####################################################
        tail_mask = 1 - soft_mask


        # pred_heatmaps: (N, K, D, H, W)
        N, K, D, H, W = pred_heatmaps.shape
        # === Soft maskï¼ˆå°±æ˜¯åŸå§‹ GT heatmapï¼‰
        # åˆå§‹åŒ– maskï¼Œå…¨ 0
        joint_mask = torch.zeros_like(pred_heatmaps)
        # ä¸ºæ¯ä¸ªæ ·æœ¬æ¿€æ´»ä¸€ä¸ª joint channel
        for b in range(N):
            joint_idx = b % K
            joint_mask[b, joint_idx] = 1.0  # åªç›‘ç£è¯¥ joint


        # === é˜²æ­¢ hand_type dim ä¸ä¸€è‡´
        if target_hand_type.dim() == 3:
            target_hand_type = target_hand_type.squeeze(1)

        # === 3D heatmap MSE lossï¼ˆç»“æ„ç›‘ç£ï¼‰
        # loss_kpt = (self.loss_kpt(pred_heatmaps, target_heatmaps) * soft_mask).mean()     # (1) 2025/04/08
        # loss_kpt = (self.loss_kpt(pred_heatmaps, target_heatmaps) * binary_mask).mean()   # (2) 2025/04/10
        # loss_kpt = (self.loss_kpt(pred_heatmaps, target_heatmaps)).mean()                   # (3) 2025/04/10-1
        # loss_kpt = (self.loss_kpt(pred_heatmaps,target_heatmaps) * 250.0 * joint_mask).mean()   # (4) 2025/04/11
        # (5) é€šé“å¯¹æ¯”æŸå¤±  , 2025/04/13
        # loss_kpt = loss_contrastive #é€šé“å¯¹æ¯”æŸå¤± 2025/04/13

        # (6) ç»„åˆ : é€šé“å¯¹æ¯”æŸå¤± + soft_mask   , 2025/04/13-1
        # loss_soft = (self.loss_kpt(pred_heatmaps, target_heatmaps) * soft_mask).mean()
        # loss_contrastive = contrastive_loss(pred_heatmaps, target_heatmaps)
        # loss_kpt = loss_soft + 0.01 * loss_contrastive

        # (7) tail regularization ï¼ˆå°¾éƒ¨æ­£åˆ™ï¼‰ , 2025/04/14
        loss_center = (self.loss_kpt(pred_heatmaps, target_heatmaps) * soft_mask).mean()
        # normalized_mask = soft_mask / 255.0
        # tail_mask = 1.0 - normalized_mask
        tail_mask = 255 - soft_mask
        loss_tail = (self.loss_kpt(pred_heatmaps, target_heatmaps) * tail_mask).mean()
        # Î± æ˜¯å°¾éƒ¨åŒºåŸŸçš„æƒ©ç½šå› å­ï¼Œå»ºè®® 0.05 ~ 0.2
        alpha = 0.2
        loss_kpt = loss_center + alpha * loss_tail

        # === (x, y) soft-argmax ä½ç½®ç›‘ç£
        pred_xy = self.soft_argmax_2d(pred_heatmaps)
        target_xy = self.soft_argmax_2d(target_heatmaps)
        loss_kpt_xy = self.loss_kpt_xy(pred_xy, target_xy)

        # === æ ¹æ·±åº¦ç›‘ç£
        # loss_root = self.loss_root(pred_root, target_root)

        # === æ‰‹ç±»å‹ç›‘ç£
        # loss_hand = self.loss_hand_type(pred_hand_type, target_hand_type)

        # === ğŸ’¡ã€æ–°å¢ã€‘ä¿¡æ¯ç†µæ­£åˆ™é¡¹ï¼ˆé¼“åŠ±çƒ­å›¾åˆ†å¸ƒç»“æ„ï¼Œä¸è¦å¡Œç¼©ï¼‰
        # æ·»åŠ  softmax å½’ä¸€åŒ–ï¼Œé˜²æ­¢ç›´æ¥ç”¨ raw logits
        # N, K, D, H, W = pred_heatmaps.shape
        # pred_softmax = F.softmax(pred_heatmaps.view(N * K, -1), dim=1).view(N, K, D, H, W)
        # entropy = -(pred_softmax * torch.log(pred_softmax + 1e-6)).mean()  # batch mean

        # print({"loss_kpt": loss_kpt, "loss_kpt_xy": loss_kpt_xy ,"loss_root": loss_root, "loss_hand_type": loss_hand})
        # return {"loss_kpt": loss_kpt, "loss_kpt_xy": loss_kpt_xy , "loss_root": loss_root, "loss_hand_type": loss_hand}

        return {
        "loss_center": loss_center,
        "loss_tail": loss_tail,
        "loss_kpt": loss_kpt,
        "loss_kpt_xy": loss_kpt_xy,
        # "loss_root": loss_root,
        # "loss_hand_type": loss_hand,
        # "loss_entropy": entropy,
        # "loss_contrast": loss_contrastive,
        # "loss_soft": loss_soft
    }


# #
# ** æµ‹è¯•**
# if __name__ == "__main__":
#     head = HandPoseHead(in_channels=512, num_joints=21, depth_size=64,heatmap_size=[64,64,64])
#     print(head)
#     x = torch.randn(5, 512, 8, 8)
#     heatmaps, root_depth, hand_type = head(x)
#     # print("è¾“å‡ºå½¢çŠ¶:", heatmaps.shape, root_depth.shape, hand_type.shape)  # (B, out_channels, H, W) è¾“å‡ºå½¢çŠ¶: torch.Size([1, 21, 32, 56, 56]) torch.Size([1, 1]) torch.Size([1, 2])
#     # print(head.predict(x))torch.tensor(np.expand_dims(heatmaps_3d, axis=0))
#     y = torch.randn(5,512,8,8)
#     heatmaps2,_,_ = head(y)
#
#     # print(f"hand_type:{hand_type}")
#     hand_type=torch.tensor([[1.,0.]])
#     head.compute_loss(pred = [heatmaps, root_depth, hand_type ],
#                       target = [heatmaps, root_depth, hand_type ])
